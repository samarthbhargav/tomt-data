#!/bin/sh
# The following lines instruct Slurm to allocate one GPU.
#SBATCH -o ./%A.out
#SBATCH -e ./%A.err
#SBATCH --partition=gpu
#SBATCH --gres=gpu:volta:2
#SBATCH -c2
#SBATCH --mem=150G
#SBATCH --time=124:00:00
# Set-up the environment.
source ${HOME}/.bashrc

# GPU
export CUDA_HOME="/usr/local/cuda-10.0"
export PATH="${CUDA_HOME}/bin:${PATH}"
export LIBRARY_PATH="${CUDA_HOME}/lib64:${LIBRARY_PATH}"
export LD_LIBRARY_PATH="${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}"


# CADORS single-hop
CUDA_VISIBLE_DEVICES=0,1 python -u multihop_dense_retrieval/mdr/retrieval/train_single.py \
    --do_train \
    --prefix ${SLURM_JOB_ID} \
    --predict_batch_size 125 \
    --model_name roberta-base \
    --train_batch_size 4 \
    --gradient_accumulation_steps 1 \
    --accumulate_gradients 1 \
    --learning_rate 2e-5 \
    --output_dir /models/TOMT/Movies/DPR \
    --train_file /data/TOMT/Movies/DPR/train_dpr.json \
    --predict_file /data/TOMT/Movies/DPR/validation_dpr.json \
    --seed 16 \
    --eval-period -1 \
    --max_c_len 512 \
    --max_q_len 512 \
    --warmup-ratio 0.1 \
    --shared-encoder \
    --num_train_epochs 0

